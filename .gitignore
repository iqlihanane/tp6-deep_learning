sans optimisers:
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.metrics import accuracy_score
import pandas as pd
import os
import cv2
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
import matplotlib.pyplot as plt
import seaborn as sns
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Pour stabilité numérique
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)
    
class MultiClassNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01, l2_lambda=0.0):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.l2_lambda = l2_lambda

        self.weights, self.biases = [], []
        np.random.seed(42)
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2. / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i + 1]))
            self.weights.append(w)
            self.biases.append(b)
def forward(self, X):
        self.activations = [X]
        self.z_values = []
        for l in range(len(self.weights) - 1):
            z = self.activations[-1] @ self.weights[l] + self.biases[l]
            a = relu(z)
            self.z_values.append(z)
            self.activations.append(a)
        z_out = self.activations[-1] @ self.weights[-1] + self.biases[-1]
        a_out = softmax(z_out)
        self.z_values.append(z_out)
        self.activations.append(a_out)
        return a_out

    def compute_loss(self, y_true, y_pred):
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        m = y_true.shape[0]
        loss = -np.sum(y_true * np.log(y_pred)) / m
        l2_penalty = (self.l2_lambda / (2 * m)) * sum(np.sum(w ** 2) for w in self.weights)
        return loss + l2_penalty

    def backward(self, y_true):
        m = y_true.shape[0]
        dZ = self.activations[-1] - y_true
        self.d_weights = []
        self.d_biases = []

        for l in reversed(range(len(self.weights))):
            a_prev = self.activations[l]
            dw = (a_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[l]
            db = np.sum(dZ, axis=0, keepdims=True) / m
            self.d_weights.insert(0, dw)
            self.d_biases.insert(0, db)
if l > 0:
                dA_prev = dZ @ self.weights[l].T
                dZ = dA_prev * relu_derivative(self.z_values[l - 1])

    def update_parameters(self):
        for l in range(len(self.weights)):
            self.weights[l] -= self.learning_rate * self.d_weights[l]
            self.biases[l] -= self.learning_rate * self.d_biases[l]

    def predict(self, X):
        probs = self.forward(X)
        return np.argmax(probs, axis=1)

    def train(self, X, y, X_val, y_val, epochs=50, batch_size=64):
        history = {
            "train_loss": [], "val_loss": [],
            "train_acc": [], "val_acc": []
        }
        for epoch in range(epochs):
            indices = np.random.permutation(X.shape[0])
            X_shuffled, y_shuffled = X[indices], y[indices]
            for i in range(0, X.shape[0], batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]
                outputs = self.forward(X_batch)
                self.backward(y_batch)
                self.update_parameters()
 train_outputs = self.forward(X)
            val_outputs = self.forward(X_val)
            train_loss = self.compute_loss(y, train_outputs)
            val_loss = self.compute_loss(y_val, val_outputs)
            train_acc = np.mean(np.argmax(train_outputs, axis=1) == np.argmax(y, axis=1))
            val_acc = np.mean(np.argmax(val_outputs, axis=1) == np.argmax(y_val, axis=1))
            history["train_loss"].append(train_loss)
            history["val_loss"].append(val_loss)
            history["train_acc"].append(train_acc)
            history["val_acc"].append(val_acc)

            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        return history["train_loss"], history["val_loss"], history["train_acc"], history["val_acc"]
# Dossier de données
DATA_DIR = r'C:\Users\PC\Desktop\iA\AMHCD_64\AMHCD_64'

try:
    labels_df = pd.read_csv(os.path.join(DATA_DIR, 'labels-map.csv'))
    assert 'image_path' in labels_df.columns and 'label' in labels_df.columns
except FileNotFoundError:
    print("CSV non trouvé. Chargement depuis l'arborescence.")
    image_paths, labels = [], []
    for label_dir in os.listdir(DATA_DIR):
        path = os.path.join(DATA_DIR, label_dir)
        if os.path.isdir(path):
            for img_file in os.listdir(path):
                image_paths.append(os.path.join(label_dir, img_file))
                labels.append(label_dir)
    labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})

assert not labels_df.empty

label_encoder = LabelEncoder()
labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])
num_classes = len(label_encoder.classes_)
def load_and_preprocess_image(image_path, target_size=(32, 32)):
    full_path = os.path.join(DATA_DIR, image_path)
    assert os.path.exists(full_path), f"Image not found: {full_path}"
    img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)
    assert img is not None, f"Failed to load image: {full_path}"
    img = cv2.resize(img, target_size)
    img = img.astype(np.float32) / 255.0
    return img.flatten()

X = np.array([load_and_preprocess_image(path) for path in labels_df['image_path']])
y = labels_df['label_encoded'].values

assert X.shape[0] == y.shape[0]
assert X.shape[1] == 32 * 32

# Séparation des jeux
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)

# One-hot
encoder = OneHotEncoder(sparse_output=False)
y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))
y_val_one_hot = encoder.transform(y_val.reshape(-1, 1))
y_test_one_hot = encoder.transform(y_test.reshape(-1, 1))

print(f"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}")

# Entraînement final complet avec Adam
nn = MultiClassNeuralNetwork(
    layer_sizes=[X_train.shape[1], 64, 32, num_classes],
    learning_rate=0.01,
    l2_lambda=0.01,
)
train_losses, val_losses, train_accuracies, val_accuracies = nn.train(
    X_train, y_train_one_hot,
    X_val, y_val_one_hot,
    epochs=100,
    batch_size=32
)

# Test final
y_pred_test = nn.predict(X_test)
y_test_decoded = y_test
y_pred_decoded = y_pred_test

print("\nRapport de classification :")
print(classification_report(y_test_decoded, y_pred_decoded, target_names=label_encoder.classes_))

cm = confusion_matrix(y_test_decoded, y_pred_decoded)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matrice de confusion')
plt.xlabel('Prédit')
plt.ylabel('Réel')
plt.tight_layout()
plt.savefig("confusion_matrix.png")
plt.show()

# Courbes perte et précision
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
ax1.plot(train_losses, label='Train Loss')
ax1.plot(val_losses, label='Validation Loss')
ax1.set_title('Perte')
ax1.legend()
ax2.plot(train_accuracies, label='Train Accuracy')
ax2.plot(val_accuracies, label='Validation Accuracy')
ax2.set_title('Précision')
ax2.legend()

plt.tight_layout()
plt.savefig("loss_accuracy_plot.png")
plt.show()
